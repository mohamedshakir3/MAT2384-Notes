\documentclass[openany]{report}
\usepackage[utf8]{inputenc}

\usepackage{stylesheets}
\usepackage{lecture_notes_styles}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\newcommand{\powerset}[0]{\mathcal{P}}

\title{MAT 2384: Numerical Methods Lecture Notes}
\author{Last Updated:}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }

\begin{document}

\maketitle

\tableofcontents

\chapter{Iterative Methods to Solve The Equation $f(x) = 0$}
Given a continous function $f$, the goal of this chapter is to estimate the solution of the equation $f(x) = 0$ in a certain interval $I$ numerically. 
\begin{theorem}[Intermediate Value Theorem]
    Let $f: [a,b] \rightarrow \real$ be a continous function. Let $y \in \real$ be any value between $f(a)$ and $f(b)$. Then there exists $z \in [a,b]$ such that $f(z) = y$.
\end{theorem}
Bolzano's Theorem is a special case of the Intermediate Value Theorem, which states
\begin{theorem}[Bolzano's Theorem]
    If a continuous function defined on an interval $I$ is sometimes positive and sometimes negative, then it must be 0 at some point. So there exists $x_0 \in I$ such that $f(x_0) = 0$.
\end{theorem}

\begin{proof}
    Without loss of generality, assume $f(a) \leq f(b)$. Let $y \in [f(a), f(b)]$. Set 
    \[S \coloneqq \{x \in [a,b]: f(x) \leq y_0\}\]
    $S$ is a subset of $[a,b]$ so it is bounded, $a \in S$ since $f(a) \leq y_0$. Therefore $S \neq \emptyset$. Thus by completeness, there exists $x_0 \coloneqq \sup S \in [a,b]$. We want $f(x_0) = y_0$. Consider the cases where $f(x_0) = y_0$, $f(x_0) < y_0$, and $f(x_0) \geq y_0$.
    \begin{itemize}
        \item \textbf{Case 1: $f(x_0) = y_0$} This case is trivial since this is the result we want. 
        \item \textbf{Case 2: $f(x_0) < y_0$} Set $\epsilon \coloneqq y_0 - f(x_0)$. Since $f$ is continous at $x_0$, $\exists \delta > 0$ such that 
        \[|x - x_0| < \delta \implies |f(x) - f(x_0)| < \epsilon\]
        Since $f(x_0) < y_0 \leq f(b)$, we can find $x > x_0$ such that $x \in [a,b]$ and $|x-x_0| < \delta$. Then $f(x) < f(x_0) + \epsilon = y_0$. So $x \in S$ by the definition of $S$, but $x > x_0$ which contradicts the fact that $x_0 = \sup S$.
        \item \textbf{Case 3: $f(x_0) > y_0$} Set $\epsilon \coloneqq f(x_0) - y_0$. Since $f$ is continous at $x_0$, $\exists \delta > 0$ such that if $x \in [a,b]$ and $|x-x_0| < \delta$, then $|f(x) - f(x_0)| < \epsilon$. So $f(x) > f(x_0) - \epsilon = y_0$ and $x_0 > a$. We can assume that $x - \delta > a$ since $\delta$ can be arbitrarly small, and we claim $x_0 - \delta$ is an upper bound for $S$. To prove this, if $x > x - \delta$, then either $|x - x_0| < \delta$, in which case $f(x) > f(x_0) - \epsilon = y_0$, or $x > x_0$ then $x \neq S$ since $x_0$ is an upper bound for $S$. Therefore, if $x >  x_0 - \delta$, then $x \neq S$, thus proving the claim. This contradicts that $x_0$ is the supremum of $S$.  
    \end{itemize}
\end{proof}
\noindent
\textbf{Example.} Prove that the equation 
\[2x^3 + 2x - 4 = 0\]
has a unique root in $[0,1]$.
\begin{proof}
    Set $f(x) \coloneqq 3x^2 + 2x - 4$, this function is continuous since it is a polynomial.We have $f(0) = -4 < 0$ and $f(1) = 1 > 0$, so by the intermediate value theorem, there exists $c \in [0,1]$ such that $f(c) = 0$. It follows that $c$ is unique since the polynomial is injective by virtue of $x^3$ and $x$ being injective. 
\end{proof}
\section{Fixed-Point Iteration}
\begin{definition}
    We say that the value $x = r$ is a \emph{fixed point} for a function $g(x)$ if $g(r) = r$.
\end{definition}
\noindent
\textbf{Example.} $g(x) = \frac{5-x^2}{4}$. $r = 1$ is a fixed-point for $g$ since $g(1) = 1$.\\[2ex]
Graphically, fixed-point of $g(x)$ correspond to the intersection of the graph of $g(x)$ and the line $y = x$. Given an equation $f(x) = 0$, we can write it under the form 
\[g(x) = x\]
by isolating one $x$ in the equation.\\[2ex]
\textbf{Example.} $3x^3 + 2x - 5 = 0$. We can write this as 
\[x = \frac{5-3x^3}{2}\]
Set $g(x) \coloneqq \frac{5-3x^3}{2}$. Then $g(x) = x$. Finding a root for $f(x) = 0$ is equivalent to finding a fixed-point for $g(x)$.\\[2ex]
\subsection{Steps to Solving Using Fixed-Point Iteration}
 Start with a first estimation $x_0$ (will be given) of the root, and form the following sequence (known as the \emph{iteration sequence})
    \[x_0, x_1 = g(x_0), x_2 = g(x_1), \dots, x_n = g(x_{n-1})\]
If this sequence converges to a value $a$, then we can prove that $a$ is a fixed-point for $g$, hence a root for $f(x) = 0$.
\begin{theorem}
    Assume that the function $g$ has a fixed-point $s$ on an interval $I$, if 
    \begin{enumerate}[label=(\roman*)]
        \item $g(x)$ is continuous on $I$
        \item $g'(x)$ is continuous on $I$
        \item $|g'(x)| < 1$ for all $x \in I$
    \end{enumerate}
    Then the iteration sequence converges.
\end{theorem}
\noindent
The steps for solving are as follows 
\begin{enumerate}
    \item Start with $f(x) = 0$
    \item Rewrite $f(x) = 0$ under the form $x = g(x)$
    \item Verify that the sequence $x_0, x_1 = g(x_0), x_2 = g(x_1), \dots, x_n = g(x_{n-1})$ converges using the above theorem (or otherwise)
    \item Compute terms of the above sequence and stop when you reach the required accuracy
\end{enumerate}
\textbf{Example.} Consider the equation 
\[x^3 + 12x - 3 = 0\]
\begin{enumerate}
    \item Prove that the equation has a unique root in $[-1.9, 1.9]$
    \item Use the Fixed-Point iteration method to estimate the value of the root to 6 decimal points starting with $x_0 = 1.8$
\end{enumerate}

\textbf{Solution:} Using the steps, we have 
\begin{enumerate}
    \item Set $f(x) \coloneqq x^3 + 12x - 3$. Since $f(x)$ is a polynomial, it is continuous, so by the intermediate value theorem, we have there exists $c \in [-1.9, 1.9]$ such that $f(c) = 0$. $f(x)$ is injective since $x^3$ and $x$ are injective, so $c$ is unique.
    \item Set $g(x) \coloneqq \frac{3-x^3}{12}$.
    \item Checking the conditions of the theorem, $g(x)$ is continuous since it is a polynomial, $g'(x) = -\frac{x^2}{4}$ is continuous since it is a polynomial. Then 
    \[|g'(x)| = \frac{x^2}{4} \leq \frac{1.9^2}{4} = 0.902 < 1\]
    Therefore, the sequence converges. 
    \item We have to calculate the terms of the iteration sequence, 
    \begin{align*}
        &x_0 = 1.8\\
        &x_1 = g(x_0) = \frac{3-1.8^2}{12} = -0.236000\\
        &x_2 = g(x_1) = \frac{3 - (0.236)^2}{12} = 0.251095\\
        &x_3 = g(x_2) = \frac{3 - (0.251095)^2}{12} = 0.24861\\
        &x_4 = g(x_3) = \frac{3 - (0.24861)^2}{12} = 0.248718\\
        &x_5 = g(x_4) = \frac{3 - (0.248718)^2}{12} = 0.248718\\
    \end{align*}
    We stop when 2 consecutive terms agree on the first 6 decimal points. So the root is $0.248718$ correct to 6 decimal points.
\end{enumerate}
\section{Newton's Method}
Newton's method is a technique for solving equations of the form $f(x) = 0$ by successive approximation. The idea is to pick an initial guess $x_0$ such that $f(x_0)$ is reasonably close to 0. We then find the equation of the line tangent to $y = f(x)$ at $x = x_0$, and determine where this tangent line intersects the $x$ axis at the new point $x_1$. So, 
\[x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}\]
We then find the equation of the line tangent to $y = f(x)$ at $x = x_1$, and repeat this process, so we have 
\[x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}\]
\textbf{Example.} Using Newton's method, estimate the value of the root of the equation 
\[x^3 + 12x - 3 = 0\]
on $[0,2]$. Start by showing that the equation has a unique root on $[0,2]$, then approximate (to 6 decimal places) with the starting point $x_0 = 1.8$.\\[2ex]
\textbf{Solution.} We have $f(x) = x^3 + 12x - 3$, and 
\[f(0) = -3 \text{ and } f(2) = 29\]
Therefore by the intermediate value theorem, there exists $c \in [0,2]$ such that $f(c) = 0$. $f(x)$ is injective since $f'(x) = 2x^2 + 12$ is striclty increasing on $[0,2]$, so $c$ is unique. Now using Newton's method, 
\[x_0 = 1.8\]
\[x_1 = x_0 - \frac{f(x_0)}{f'(x_0)} = 0.675138\]
\[x_2 = x_1 - \frac{f(x_1)}{f'(x_1)} = 0.270469\]
\[x_3 = x_2 - \frac{f(x_2)}{f'(x_2)} = 0.248748\]
\[x_4 = x_3 - \frac{f(x_3)}{f'(x_3)} = 0.248718\]
\[x_5 = x_4 - \frac{f(x_4)}{f'(x_4)} = 0.248718\]
Therefore, the our root is $0.248718$ correct to 6 decimal places.\\[3ex]
\textbf{Example.} Consider the equation 
\[x^3 - 2x - 5 = 0\]
\begin{enumerate}[label=(\roman*)]
    \item Prove that the equation has a unique root in $[2,3]$
    \item Starting with $x_0 = 3$, estimate the root of the equation to 6 decimal places using Newton's method.
\end{enumerate}
\textbf{Solution.} 
\begin{enumerate}[label=(\roman*)]
    \item We have $f(2) = -1$ and $f(3) = 16$, therefore by the intermediate value theorem there exists $c \in [2,3]$ such that $f(c) = 0$. $f'(x) = 3x^2 - 2$ is injective since if $f(x_1) = f(x_2)$, then we have
    \begin{align*}
        &f(x_1) = f(x_2)\\
        \implies& x_1^3 - 2x_1 - 5 = x_2^3 - 2x_2 - 5\\
        \implies& x_1^3 - 2x_1 = x_2^3 - 2x_2\\
    \end{align*} 
    Then $x^3$ and $x$ are injective functions, so we must have that $x_1 = x_2$ and therefore the root is unique. Alternatively, we can look at the derivative on its interval, 
    \begin{align*}
        2 \leq x \leq 3\\
        4 \leq x^2 \leq 9\\
        12 \leq 3x^2 \leq 27\\
        10 \leq 3x^2 - 2 \leq 25\\
    \end{align*}
    Therefore the derivative is positive so the function is strictly increasing, and thus injective.
    \item Starting with $x_0 = 3$, using Newton's method we get 
    \[x_1 = x_0 - \frac{f(x_0)}{f'(x_0)} = 3 - \frac{3^3 - 2(3) - 5}{3(3)^2 - 2} = 2.600000\]
    \[x_2 = 2.6 - \frac{f(2.6)}{f'(2.6)} = 2.127197\]
    \[x_3 x_2 - \frac{f(2.127197)}{f'(2.127197)} = 2.0945136\]
    \[x_4 = x_3 - \frac{f(2.094552)}{f'(2.094552)} = 2.094552\]
    \[x_5 = x_4 - \frac{f(x_4)}{f'(x_4)} = 2.094551 \]
    \[x_6 = x_5 - \frac{f(x_5)}{f'(x_5)} = 2.094551\]
    Therefore, we have $x \approx 2.094551$ correct to 6 decimal places.
\end{enumerate}
\noindent
\textbf{Example.} Use Newton's Method with $x_0 = 2$ to estimate the value of $\sqrt[3]{7.9}$ correct to 6 decimal places.\\[2ex]
\textbf{Solution.} We can set $x \coloneqq \sqrt[3]{7.9}$, so we have $x^3 - 7.9 = 0$. Then this can be solved the same as the previous examples.
\section{The Secant Method}
The tangent line to the curve of $y = f(x)$ with the point of tangency $(x0, f(x0)$ was used in Newton's approach. The graph of the tangent line about $x = \alpha$ is essentially the same as the graph of $y = f(x)$ when $x_0 \approx \alpha$. The root of the tangent line was used to approximate $\alpha$. Consider employing an approximating line based on interpolation. Given 2 root estimations $x_0$ and $x_1$, then we have a linear function
\[q(x) = a_0 + a_1x\]
with $q(x_0) = f(x_0)$, and $q(x_1) = f(x_1)$. This line is alo known as the secant line, with the formula 
\[q(x) = \frac{(x_1 - x)f(x_0) + (x-x_0)f(x_1)}{x_1 - x_0}\]
The linear equation $q(x) = 0$ with the root denoted by $x_2$ is given by
\[x_2 = x_1 - f(x_1) \cdot \frac{x_1 - x_0}{f(x_1) - f(x_0)}\]
This equation can now be employed for every term in the sequence, 
\[x_{n+1} = x_n - f(x_n) \cdot \frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1}) } \]
\textbf{Example.} Use the secant method with $x_0 = 2$ and $x_1 = 1.9$ to estimate the root of the equation to 6 decimal places 
\[2\sin x - x = 0\]
\textbf{Solution.} We have $f(x) = 2 \sin x - x$, we can start calculating the terms of the sequnce 
\[x_2 = x_1 - f(x_1)\frac{x_1 - x_0}{f(x_1) - f(x_0)} = 1.9 - (2\sin(1.9) - 1.9)\frac{1.9 - 2}{(2 \sin(1.9) - 1.9) - (2 \sin (2) - 2)} = 1.895747\]
\[x_3 = x_2 - f(x_2)\frac{x_2 - x_1}{f(x_2) - f(x_1)} = 1.895747\]
Therefore the root is $x \approx 1.895747$ correct to 6 decimal places.
\chapter{Interpolation}
\section{Generalities}
Given a set of $n+1$ data points $(x_0, f_0), \ldots, (x_n, f_n)$ where 
\[f_i = f(x_i)\]
for some unkown function $f$, the goal is to  find a \emph{polynomial} function of degree $n$, say $p_n(x)$, where its graph goes through all the datapoints. We then can use the approximation $f(x) \approx p_n(x)$.
\begin{theorem}
    Given a collection of $n+1$ data points $(x_0, f_0), \ldots, (x_n, f_n)$ in the cartesian plane such that 
    \[x_0 < x_1 < x_2 < \cdots < x_n\]
    Then there exists a unique polynomial of degree $\leq n$ such that 
    \[p_n(x_i) = f_i \ \forall i \in \{0, 1, \ldots, n\}\]
\end{theorem}
If we use the approximation $f(x) \approx p_n(x)$, then the absolute error ($|f(x) - p_n(x)|$) is given by the following theorem. 
\begin{theorem}[Error Formula]
    The error formula with the above notation is 
    \[|f(x) - p_n(x)| = |(x-x_0)(x-x_1)\cdots(x-x_n)\frac{f^{(n+1)}(t)}{(n+1)!}\]
\end{theorem}
\section{Lagrange Interpolation}
Recall that our objective is approximate the function $f(x)$ given $n+1$ datapoints of the form $(x_0, f_0), (x_1,f_1), \ldots, (x_n,f_n)$. Lagrange proved that the following polynomial goes through all of these points 
\[p_n(x) = L_0(x)f_0 + L_1(x)f_1 + \cdots L_n(x)f_n\]
Where 
\begin{align*}
    L_0 &= \frac{(x-x_1)(x-x_2)\cdots(x-x_n)}{(x_0-x_1)(x_0-x_2)\cdots(x_0-x_n)}  \\
    L_1 &= \frac{(x-x_0)(x-x_2)\cdots(x-x_n)}{(x_1-x_0)(x_1-x_2)\cdots(x_1-x_n)}\\
    L_2 &= \frac{(x-x_0)(x-x_1)(x-x_3)\cdots(x-x_n)}{(x_2-x_0)(x_2-x_1)(x_2-x_3)\cdots(x_2-x_n)}
\end{align*}
\noindent
\textbf{Example.} Consider the following 3 data points 
\[(0.7, 2.2),(1.3,3.1),(1.6,4)\]
where $f_i = f(x_i)$ for an unknown function $f$. 
\begin{enumerate}[label=(\roman*)]
    \item Find the Lagrange interpolation polynomial $p_2(x)$. 
    \item Interpolate $f(1)$. 
    \item If $2 \leq |f'''(t)| \leq 3$ for all $t \in [0.7, 1.6]$, find an upper bound for the error in the approximation $f(1) \approx p_2(1)$.
\end{enumerate}
\textbf{Solution.} 
\begin{enumerate}[label=(\roman*)]
    \item We know that our polynomial is of the form
    \[p_2(x) = L_0(x) f_0 + L_1(x)f_1 + L_2(x)f_2\]
    Where $f_0,f_1,f_2$ are given. We can calculate the $L_i$'s as follows
    \begin{align*}
        L_0 &= \frac{(x - x_1)(x-x_2)}{(x_0 - x_1)(x_0-x_2)}\\
        &= \frac{(x-1.3)(x-1.6)}{(0.7 - 1.3)(0.7-1.6)}\\
        &= 1.519x^2 - 5.3704x + 3.8519\\
        L_1 &= \frac{(x - x_0)(x-x_2)}{(x_1 - x_0)(x_1-x_2)}\\
        &= \frac{(x-0.7)(x-1.6)}{(1.3-0.7)(1.3-1.6)}\\
        &= -5.6667x^2 + 12.77778x - 6.22222\\
        L_1 &= \frac{(x - x_0)(x-x_1)}{(x_2 - x_0)(x_2-x_1)}\\
        &= \frac{(x-0.7)(x-1.3)}{(1.6-0.7)(1.6-1.3)}\\
        &= 3.7037x^2 - 7.4074x + 3.3704
    \end{align*}
    So our polynomial is
    \begin{align*}z
        p_2(x) &= (1.519x^2 - 5.3704x + 3.8519)(2.2) + (-5.6667x^2 + 12.77778x - 6.22222)(3.1) \\
        &+ (3.7037x^2 - 7.4074x + 3.3704)(4)\\
        &= 1.66667x - 1.83333x + 2.66667
    \end{align*}
    We can check that this polynomial does go through all our points.
    \item We can interpolate $f(1)$ by plugging in $x = 1$ into our polynomial, so we have
    \[f(1) \approx p_2(x) = 2.50000\]
    \item We can use the error formula to find an upper bound for the error,
    \begin{align*}
        |f(1) - p_2(1)| &= \left|(1 - 0.7)(1-0.13)(1-1.6)\frac{f'''(t)}{3!}\right|
        &= 0.009 |f'''(t)|\\
        0.0009(2) = 0.0018 &\leq |f(1) - p_2(1)| \leq 0.009(3) = 0.027
    \end{align*}
    Therefore our lower bound is 0.0018 and our upper bound is 0.027.
\end{enumerate}
\textbf{Example.} Consider the 4 points $(x_i,f_i)$, 
\[(0,1), \ (1,0.765) \ (2,0.224), \ (3, -0.260) \]
\begin{enumerate}[label=(\roman*)]
    \item Find the Interpolation polynomial $p_3(x)$ using your Lagrange. Round your answer to 3 decimal places.
    \item Interpolate a value for $f(2.5)$
    \item GIven that $0.75 \leq |f^{(4)}(t)| \leq 1.17$ for any $t \in [0,3]$, give an upper and a lower bound for the error in the approximation $f(2.5) \approx p_3(2.5)$.
\end{enumerate}
\textbf{Solution.} 
\begin{enumerate}[label=(\roman*)]
    \item We start with the Lagrange polynomial
    \[p_3(x) = L_0(x)f_0 + L_1(x)f_1 + L_2(x)f_2 + L_3(x)f_3\]
    \begin{align*}
        L_0(x) &= \frac{(x-x_1)(x-x_2)(x-x_3)}{(x_0-x_1)(x_0-x_2)(x_0-x_3)}\\
        &= \frac{(x-1)(x-2)(x-3)}{(0-1)(0-2)(0-3)}\\
        &= -0.167x^3 + x^2 - 1.833x + 1\\
        L_1(x) &= \frac{(x-x_0)(x-x_2)(x-x_3)}{(x_1-x_0)(x_1-x_2)(x_1-x_3)}\\
        &= \frac{(x-0)(x-2)(x-3)}{(1-0)(1-2)(1-3)}\\
        &= 0.5x^3 - 2.5x^2 + 3x\\
        L_2(x) &= \frac{(x-x_0)(x-x_1)(x-x_2)}{(x_2-x_0)(x_2-x_1)(x_2-x_3)}\\
        &= \frac{(x-0)(x-1)(x-3)}{(2-0)(2-1)(2-3)}\\
        &= -0.5x^3 + 2x^2 - 1.5x\\
        L_3(x) &= \frac{(x-x_0)(x-x_1)(x-x_2)}{(x_3-x_0)(x_3-x_1)(x_3-x_2)}\\
        &= \frac{(x-  0)(x-1)(x-2)}{(3-0)(3-1)(3-2)}\\
        &= 0.167x^3 - 0.5x^2 + 0.33x
    \end{align*}
    Then,
    \begin{align*}
        p_3(x) &= (-0.167x^3 + x^2 - 1.833x + 1)(1) + ( 0.5x^3 - 2.5x^2 + 3x)(0.765)\\
        &+ (-0.5x^3 + 2x^2 - 1.5x)(0.224)\\
        &+ (0.167x^3 - 0.5x^2 + 0.33x)(-0.260)\\
        &= 0.061x^3 - 0.335x^2 + 0.040x + 1  
    \end{align*} 
    \item Then we can calculate $f(2.5) \approx p_3(2.5)$
    \[p_3(2.5) = 0.061(2.5)^3 - 0.335(2.5)^2 + 0.040(2.5) + 1 = -0.048\]
    \item Then the error is given by 
    \begin{align*}
        |f(2.5) - p_3(2.5)| &= \left|(2.5-0)(2.5-1)(2.5-2)(2.5-3)\frac{f^{(4)}(t)}{4!}\right|\\
        &= 0.039|f^{(4)}(t)|\\
    \end{align*}
    Then using $0.75 \leq |f^{(4)}(t)| \leq 1.17$, we have
    \[0.039(0.75) \leq |f(2.5) - p_3(2.5)| \leq 0.039(1.17)\]
\end{enumerate}

\section{Newton's Divided Difference Interpolation Polynomial}
Similar to Lagrange Interpolation, we start with $n$ datapoints $(x_0,f_0),\ldots,(x_n,f_n)$ where $f_i = f(x_i)$ for some unknown function $f$. 
\begin{definition}
    Given a node $x_i$, 
    \begin{enumerate}
        \item The \emph{first divided difference} at $x_i$ is defined as 
        \[f(x_i,x_{i+1}) = \frac{f_{i+1}- f_i}{x_{i+1} - x_i}\]
        \item The \emph{second divided difference} at $x_i$ is 
        \[f(x_i, x_{i+1}, x_{i+2}) = \frac{f(x_{i+1}, x_{i+2}) - f(x_i,x_{i+1})}{x_{i+2}-x_i}\]
        \item In general, the $k$th divided difference at $x_i$ is 
        \[f(x_i, x_{i+1}, \ldots x_{i+k}) = \frac{f(x_{i+1},\ldots, x_{i+k}) - f(x_i, \ldots x_{i+k-1})}{x_{i+k}-x_i}\]
    \end{enumerate}
\end{definition}

Then, we can define Newton's Interpolation polynomial as 
\begin{align*}
    p_n(x) &= f_0 + f(x_0,x_1)(x-x_0)\\
    &+ f(x_0,x_1,x_2)(x-x_0)(x-x_1) + \cdots\\
    &+ f(x_0,\ldots,x_n)(x-x_0)(x-x_1)\cdots(x - x_n)
\end{align*}
\textbf{Example.} Given 3 datapoints, 
\[(1.2, 4.5), (1.7,5.9), (2.1,7.4) \]
Calculate the interpolation polynomial using Newton's Divided Difference method and approximate $f(1.8)$.\\[2ex]
\textbf{Solution.} We start by calculating \emph{all} the first divided differences,
\[f(x_0, x_1) = \frac{f_1 - f_0}{x_1 - x_0} = \frac{5.9-4.5}{1.7-1.2} = 2.8\]
\[f(x_1,x_2) = \frac{f_2-f_1}{x_2-x_1}=\frac{7.4-5.9}{2.1-1.7} = 3.75\]
Now we can calculate all the second divided differences, in this case there is only one 
\[f(x_0,x_1,x_2) = \frac{f(x_1, x_2) - f(x_0,x_1)}{x_2-x_0} = \frac{3.75 - 2.8}{2.1 - 1.2} = 1.05556\]
So the Newton's Interpolation polynomial is
\begin{align*}
    p_2(x) &= f_0 + f(x_0,x_1)(x-x_0) + f(x_0,x_1,x_2)(x-x_0)(x-x_1)\\
    &= 4.5 + 2.8(x-1.2) + 1.05556(x-1.2)(x-1.7)\\
    &= 1.05556x^2 - 0.26111x + 3.29333
\end{align*}
Then we can use this polynomial to approximate $f(1.8)$,
\[f(1.8) \approx p_2(1.8) = 6.24333\]

\chapter{Numerical Integration}

The fundemental theorem of calculus states that if $f$ is continuous on $[a,b]$, then
\[\int_{a}^{b} f(x)dx = F(b) - F(a)\]
where $F'(x) = f(x)$. In practice, it is often difficult to find an antiderivative of $f(x)$, so the goal of this chapter is to explore numerical methods to estimate the value of the integral.

\section{Midpoint Method}
The idea is to divide the interval $[a,b]$ into $n$ subintervals of equal length, and approximate the function $f(x)$ with the constant function $y = f(x_i^*)$ on $[x_i, x_{i+1}]$ where 
\[x_i^* = \frac{x_i + x_{i+1}}{2}\]
is the mid point of the sub interval. The length of each subinterval is
\[h = \frac{b-a}{n}\]
So we approximate the integral with 
\[\int_a^b f(x)dx \approx h[f(x_1^*) + f(x_2^*) + \cdots + f(x_n^*)]\]
The error in the midpoint rule satifies the following inequality 
\[|E_m| \leq \frac{M(b-a)^3}{24n^2}\]
where $M$ is an upper bound for $|f''(t)|$ for $t \in [a,b]$. To find an upperbound $|f''(x)|$, it might be useful to compute $f'''(x)$ to see if $f''(x)$ is decreasing or increasing. If $f''(x)$ is decreasing, then we can use $f''(a)$ as an upper bound, and if $f''(x)$ is increasing, then we can use $f''(b)$ as an upper bound.\\[2ex]
\textbf{Example.} Consider the integral 
\[I = \int_1^2 x \ln x dx\]
Use the midpoint rule to estimate the value of $I$ with a maximum error of $0.001$. \\[2ex]
\textbf{Solution.} We first need to divide the interval $[1,2]$ into $n$ subintervals of length $h = \frac{1}{n}$. Then we can calculate the value for $n$ to meet the error requirement,
\[f(x) = x \ln x\]
\[f'(x) = \ln x + 1\]
\[f''(x) = \frac{1}{x}\]
\[f'''(x) = -\frac{1}{x^2}\]
The third derivative is negative on $[1,2]$ so $f''(x)$ is decreasing, therefore 
\[f''(2) \leq f''(x) \leq f''(1) \implies |f''(x)| \leq 1\]
So $M = 1$, then 
\[|E_m| \leq \frac{1 (2-1)^3}{24n^2} \leq 0.001 \implies n \geq 6.45\]
We take $n = 7$ to have an error at most $0.001$. Then we can calculate $h$ 
\[h = \frac{b-a}{n} = \frac{1}{7}\]
\[x_1^* = \frac{1 + 8/7}{2} = \frac{15}{14}\]
\[x_2^* = x_1^* + \frac{1}{7} = \frac{17}{14}\]
\[x_3^* = x_2^* + \frac{1}{7} = \frac{19}{14}\]
\[x_4^* = x_3^* + \frac{1}{7} = \frac{21}{14}\]
\[x_5^* = x_4^* + \frac{1}{7} = \frac{23}{14}\]
\[x_6^* = x_5^* + \frac{1}{7} = \frac{25}{14}\]
\[x_7^* = x_6^* + \frac{1}{7} = \frac{27}{14}\]
Then we can calculate the approximation of the integral,
\begin{align*}
    \int_1^2 x\ln x &\approx h[f(x_1^*) + \cdots + f(x_7^*)]\\
    &= \frac{1}{7}\left[\frac{15}{14} \ln \left(\frac{15}{14}\right) + \cdots + \frac{27}{14}\ln\left(\frac{27}{14}\right)\right]\\
    &= 0.63571
\end{align*}
\textbf{Example.} Estimate the value of the following integral uing the midpoint rule with a maximal absolute error of $0.001$.
\[I = \int_0^{0.5}x\cos x dx\]
\textbf{Solution.} We have to first find $n$ 
\[f(x) = x\cos x\]
\[f'(x) = \cos x - x\sin x\]
\[f''(x) = -\sin x - \sin x - x\cos x = -2\sin x - x\cos x\]
We can see that 
\begin{align*}
    |-2\sin x - x\cos x| &\leq |-2\sin x| + |-x\cos x|\\
    &= 2|\sin x| + |x||\cos x|\\
    &\leq 2 + |x|\\
    &\leq 2.5   
\end{align*}
Thus $M = 2.5$, then by the error formula we have
\begin{align*}
    |E_m| &\leq \frac{2.5(0.5-0)}{24n^2} \leq 0.001  \implies n \geq \sqrt{\frac{2.5(0.5)^3}{24(0.001)}} = 2.79
\end{align*}
We take $n = 3$. Then we can calculate $h$,
\[h = \frac{0.5 -0}{3} = \frac{1}{6}\]
\[x_1^* = \frac{0 + 1/6}{2} = \frac{1}{12}\]
\[x_2^* = x_1^* + \frac{1}{6} = \frac{1}{4}\]
\[x_3^* = x_2^* + \frac{1}{6} = \frac{5}{12}\]
By the midpoint rule, we have 
\begin{align*}
    \int_{0}^{0.5} x\cos x dx &\approx h[f(x_1^*) + f(x_2^*) + f(x_3^*)]\\
    &= \frac{1}{6}\left[\frac{1}{12}\cos \left(\frac{1}{12}\right) + \frac{1}{4}\cos \left(\frac{1}{4}\right) + \frac{5}{12}\cos \left(\frac{5}{12}\right)\right]
\end{align*}

\section{Trapezoidal Rule}
Similar to the mid-point rule, we start by dividing the interval for the integral 
\[\int_a^b f(x)dx\]
into $n$ subintervals $(x_0,x_1)$, $(x_1,x_2)$,$\ldots$,$(x_{n-1}, x_n)$. The length of the intervals $h$ is 
\[h = \frac{b-a}{n}\]
we approximate $f(x)$ with the linear function to calculate the areas of the trapezoids that are formed by the graph of $f(x)$ and the $x$-axis. So 
\[\int_a^b f(x)dx\approx \frac{h}{2}\left[f(a) + 2f(x_1) + 2f(x_2) + \cdots + 2f(x_{n-1}) + f(x_n)\right]\]
\begin{theorem}
    The absolute error $|E_T|$ in the Trapezoidal rule satisfies
    \[|E_T| \leq \frac{M(b-a)^3}{12n^2}\]
\end{theorem}

\textbf{Example.} Use the Trapezoidal rule with a maximal absolute error of $0.01$ to estimate the value of the integral
\[\int_0^1 e^{-x^2}dx\]
\textbf{Solution.} First we compute $n$, 
\[f(x) = e^{-x^2}\]
\[f'(x) = -2xe^{-x^2}\]
\[f''(x) = -2e^{-x^2} + 4x^2e^{-x^2}\]
\[f''(x) = -2e^{-x^2} + 4x^2e^{-x^2}\]
\[f'''(x) = 8xe^{-x^2} + (-2 + 4x^2)(-2xe^{-x^2}) = e^{-x^2}4x(3-2x^2)\]
From the third derivative we see that $f'''(x)$ is always positive on $[0,1]$, so $f''(x)$ is increasing, therefore 
\[f''(0) \leq f''(x) \leq f''(1) \implies -2 \leq f''(x) \leq -2e^{-1} + 4e^{-1}\]
Thus we can take $M =2$. Then by the error formula we have
\[\frac{2}{12n^2} \leq 0.01 \implies n \geq \sqrt{\frac{1}{6(0.01)}} \approx 4.08\]
Then we take $n = 5$. So our length of each subinterval is
\[h = \frac{1 -0}{5} = 0.2\]
Now we can approximate the integral 
\begin{align*}
    \int_0^1 e^{-x^2}dx &\approx \frac{0.2}{2}\left[f(0) + 2f(0.2) + 2f(0.4) +2f(0.6) + 2f(0.8) + f(1)\right]\\
    &=0.1\left(1 + 2e^{-0.2^2} + 2e^{-0.4^2} + 2e^{-0.6^2} + 2e^{-0.8^2} + 2e^{-1}\right)
\end{align*}

\section{Simpson's Rule}

We start by subdividing $[a,b]$ into an \emph{even} number of subintervals. The idea is to estimate the function $f(x)$ in every subinterval with a polynomial of degree 2. 
\[\int_a^b f(x) \approx \frac{h}{3}\left[f(a) + 4(x_1) + 2f(x_2) + 4f(x_3) + \cdots + f(b)\right]\]
where $h = \frac{b-a}{n}$ and $n$ is the number of subintervals. The error in Simpson's rule is given by
\[|E_S| \leq \frac{M(b-a)^5}{180n^4}\]
Where $M$ is the upperbound for the fourth derivative of $f(x)$.\\[2ex]
\textbf{Example.} Using Simpson's rule, with a maximal error of $0.001$, estimate the value of the integral
\[\int_{0.5}^{1.5}x^2\ln x dx\]
\textbf{Solution.} We start by computing $n$,
\[f(x) = x^2\ln x\]
\[f'(x) = 2x\ln x + x\]
\[f''(x) = 2\ln x + 3\]
\[f'''(x) = \frac{2}{x}\]
\[f^{(4)}(x) = -\frac{2}{x^2}\]
\[f^{(5)}(x) = \frac{4}{x^3}\]
The fifth derivative is always positive so $f^{(4)}$ is increasing, therefore
\[f^{(4)}(0.5) \leq f^{(4)}(x) \leq f^{(4)}(1.5) \implies |f^{(4)}(x)| \leq 8\]
So we can take $M = 8$. Then by the error formula we have
\[\frac{8(1.5-0.5)}{180n^4} \implies n \geq \sqrt[4]{\frac{8}{180(0.001)}} \approx 2.58\]
We need an even $n$ so we take $n=4$. Then we can calculate $h$,
\[h = \frac{1}{4}\]
\[x_0 = 0.5\]
\[x_1 = 0.5 + \frac{1}{4} = 0.75\]
\[x_2 = 0.75 + \frac{1}{4} = 1\]
\[x_3 = 1 + \frac{1}{4} = 1.25\]
\[x_4 = 1.25 + \frac{1}{4} = 1.5\]
Then we can approximate the integral,
\begin{align*}
    \int_{0.5}^{1.5} x^2\ln x dx &\approx \frac{0.25}{3}\left[f(0.5) + 4f(0.75) + 2f(1) + 4f(1.25) + f(1.5)\right]\\
    &= \frac{1}{12}\left[0.5^2\ln 0.5 + 4(0.75)^2\ln 0.75 + 2(1)^2\ln 1 + 4(1.25)^2\ln 1.25 + (1.5)^2\ln 1.5\right]\\
    &\approx 0.123915
\end{align*}

\section{Gaussian Quadrature}
The Gaussian Quadrature of order $n$ consists of estimating the integral 
\[\int_{-1}^{1}f(t)dt\]
using an expression of the form 
\[\int_{-1}^{1}f(t)dt \approx w_1f(t_1) + \cdots w_nf(t_n)\]
where $t_1, \ldots, t_n$  are not necessarily equidistant and are called the \emph{nodes} and $w_1,\cdots w_n$ are constants called the coefficients. The approximation becomes an equality if $f(t)$ is a polynomial of degree $2n-1$. In general, to convert $\int_a^bf(x)dx$ to $\int_{-1}^{1}g(t)dt$, we use the following change of variables
\[x = \frac{b-a}{2}t + \frac{b+a}{2}\]
We then use a table of values to find the nodes and coefficients.
\begin{center}
    \begin{tabular}{c|cc}
        Order $n$ & Nodes $t_i$ & Coefficients $w_i$\\
        \hline
        1 & 0 & 2\\
        \hline
        2 & $-0.5773502692$ & 1\\
        & $0.5773502692$ & 1\\
        \hline
        & $-0.7745966692$ & $0.555555556$\\
        3 & 0 & 0.888888889\\
        & $0.7745966692$ & $0.555555556$\\
        \hline
        & -0.8611363116 &0.3478548451\\
        4&-0.3399810436& 0.6521451549\\
        &0.3399810436 &0.6521451549\\
        &0.8611363116 &0.3478548451\\
        \hline
        &-0.9061798459 &0.2369268850\\
        &-0.5384693101& 0.4786286705\\
        5 &0.0 &0.5688888889\\
        &0.5384693101 &0.4786286705\\
        &0.9061798459& 0.2369268850\\
    \end{tabular}
\end{center}
\noindent
\textbf{Example.} Use Gaussian Quadrature of order 4 to estimate the value of 
\[\int_{0}^1\sin(x^2)dx\]
\textbf{Solution.} First we substitute $x$ with 
\[x = \frac{b-a}{2}t + \frac{b+a}{2} = \frac{1}{2}t +\frac{1}{2}\]
\[\frac{dx}{dt} = \frac{1}{2} \implies dx = \frac{dt}{2}\]
\[\int_0^1 \sin(x^2) = \int_{-1}^1 \sin \left(\frac{(t+1)^2}{4}\right) \frac{1}{2}dt\]
Then we can use Gaussian Quadrature of order 4 to estimate the value of the integral,
\begin{align*}
    \int_{-1}^1 \sin \left(\frac{(t+1)^2}{4}\right) \frac{1}{2}dt &\approx w_1f(t_1) + w_2f(t_2) + w_3(t_3) + w_4f(t_4)\\
    &= 0.3479f(-0.8611) + 0.6521f(-0.3399)\\
    &+0.6521f(0.3399) + 0.3479f(0.8611)\\
\end{align*}




\chapter{Numerical Methods to Solving First-Order IVP's}

Given a first-order IVP 
\[y' = f(x,y), y(x_0) = y_0\]
The goal of this chapter is to explore techniques that allow us to estimate values of the function $y$. 

\section{Euler Method}

Given a step size between our $x$ values, Euler's method uses the formula 
\[x_{n+1} = x_n + h\]
\[y_{n+1} = y_n + hf(x_n, y_n)\]
where $h$ is the given step size, and $y' = f(x,y)$ for the differential equation. \\[2ex]
\textbf{Example.} Consider the IVP 
\[y' = 2x + y, \ y(0) = -1\]
Use Euler's method with a step size $h = 0.2$ to estimate the values of the function $y$ on the interval $[0,0.6]$. \\[1ex]

\textbf{Solution.} We have $f(x,y) = 2x + y$, $x_0 = 0$, $y_0 = -1$, and $h = 0.2$. Now we can caluclate each step 
\[x_1 = x_0 + h = 0.2\]
\[y_1 = y_0 + h f(x_0,y_0) = -1 + 0.2(-1) = -1.2\]
\[x_2 = x_1 + h = 0.4\]
\[y_2 = y_1 + hf(x_1,y_1) = -1.2 + 0.2(2(0.2) - 1.2) = -1.36\]
\[x_3 = x_2 + h = 0.6\]
\[y_3 = y_2 + hf(x_2,y_2) = -1.36 + 0.2(2(0.4) - 1.36) = -1.472\]


\section{Improved Euler Method}

The improved Euler method consists of using Euler's method to "predict" the value for $y$, then "correct" it at each step to have a more accurate value. Given a first order IVP 
\[y' = f(x,y), \ y(x_0) = y_0\]
We have the $x$ values at each step 
\[x_{n+1} = x_n + h\]
Then the $y$ values are 
\[y^c_{n+1} = y_n^c + \frac{h}{2}\left[f(x_n,y_n^c) + f(x_{n+1}, y^p_{n+1})\right]\]
where $y^p$ is the predicted $y$ value obtained from the standard Euler method.\\[2ex]

\noindent
\textbf{Example.} Consider the previous IVP with the improved Euler method, 
\[y' = 2x+y, \ y(0) = -1\]
We're given $f(x,y) = 2x + y$, $h = 0.2$, $y_0 = -1$, and $x_0 = 0$, then 
\[x_1 = x_0 + h = 0.2\]
\[y_1^p = y_0 + h f(x_0, y_0) = -1 + 0.2(-1) = -1.2\] 
\[y_1^c = y_0 + \frac{h}{2}\left[f(x_0,y_0) + f(x_1,y_1^p)\right] = -1 + 0.1 \left[-1 + 2(0.2) - 1.2\right] = -1.18\]
\[x_2 = x_1 + h = 0.4\]
\[y_2^p = y_1^c + h f(x_1, y_1^c) = -1.18 + 0.2(2(0.2) - 1.18) = -1.336\]
\[y_2^c = y_1^c + \frac{h}{2}\left[f(x_1,y_1^c) + f(x_2,y_2^p)\right] = -1.18 + 0.1\left[2(0.2) - 1.18 + 2(0.4)\right] = -1.3116\]
\[x_3 = x_2 + h = 0.6\]
\[y_3^p = y_2^c + hf(x_2, y_2^c) = -1.3116 + 0.2(2(0.4) - 1.3116) = -1.41392\]
\[y_3^c = -1.3116 + 0.1\left[2(0.4) - 1.18 + 2(0.6) - 1.413192\right] = -1.384152\]

\section{Runge-Kutta Method of Order 4}

Given a first order ODE 
\[y' = f(x,y); \ y(x_0) = y_0\]
In the improved Euler method, we had "2 steps" with "predicting" and "correcting" each term. The Runge-Kutta method of order 4 concists of "4 steps" to compute $y_{n+1}$. The formula is as follows. Note that the step size is given as $h$, 
\[y' = f(x,y) ; \ y(x_0) = y_0\]
\[x_{n+1} = x_n + h\]
\[k_1 = hf(x_n,y_n)\]
\[k_2 = hf\left(x_n + \frac{1}{2}h, y_n + \frac{1}{2}k_1\right)\]
\[k_3 = hf\left(x_n + \frac{1}{2}h, y_n + \frac{1}{2}k_2\right)\]
\[k_4 = hf\left(x_n + h, y_n + k_3\right)\]
\[y_{n+1} = y_n + \frac{1}{6}(k_1 + 2k_2 + 2k3 + k_4)\]

\noindent
\textbf{Example.} Conisder the IVP 
\[y' = y - x^2 + 1; \ y(0) = \frac{1}{2}\]
Use Runge-Kutta Method of Order 4 to estimate the values of the function $y$, on the interval $[0,1]$, using a step size $h = 0.5$.\\[-2ex]

\noindent
\textbf{Solution.}
We have $x_0 = 0$, and $y_0 = \frac{1}{2}$, and $f(x,y) = y -x^2 + 1$. Now we can compute with a step size $h = 0.5$,
\begin{align*}
    x_1 &= x_0 + h = 0.5 \\
    k_1 &= hf(x_0, y_0) = 0.5(\frac{1}{2} - 0 + 1) = 0.75\\
    k_2 &= hf\left(x_0 + \frac{1}{2}h, y_0 + \frac{1}{2}k_1\right) = 0.5f(0.25,0.875) = 0.90625\\
    k_3 &= hf\left(x_0 + \frac{h}{2}, y_0 + \frac{1}{2}k_2\right) = 0.5f(0.25, 0.953125) = 0.9453125\\
    k_3 &= hf\left(x_0 + h, y_0 + k_3\right) = 0.5f(0.5, 1.4453125) = 1.09765625\\
\end{align*}
Then, 
\[y_1 = y_0 + \frac{1}{6}(k_1 + 2k_2 + 3k_2 + k_4) = 1.4251306\]
So we get the new point 
\[(x_1,y_1) = (0.5, 1.4251302)\]
Then we continue 
\begin{align*}
    x_2 &= x_1 + h = 1\\
    k_1 &= hf(x_1, y_1) = 1.087561 \\
    k_2 &= hf\left(x_1 + \frac{1}{2}h, y_1 + \frac{1}{2}k_1\right) = 1.2032014\\
    k_3 &= hf\left(x_1 + \frac{1}{2}h, y_1 + \frac{1}{2}k_2\right) = 1.2321167\\
    k_4 &= hf(x_1 + h, y_1 + k_3) = 1.3286235\\
    y_2 &= y_1 + \frac{1}{6}(k_1 + 2k_2 + 3k_2 + k_4) = 2.6396027
\end{align*}
This gives us our final point 
\[(x_2, y_2) = (1, 2.6396027)\]
\end{document} 